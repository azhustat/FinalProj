%% STAT 215B, Spring 2012
%% Final Project
%% Note: need to use XeLaTeX to compile

\documentclass[11pt]{article}
\usepackage[letterpaper, hmargin={1in,1in}, vmargin={1in,1in}, noheadfoot]{geometry}
\usepackage{listings} % for including source code

%\usepackage[usenames,dvipsnames]{color}

\usepackage{hyperref, graphicx}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{ marvosym }

%%% for displaying Chinese
\usepackage{fontspec,xltxtra,xunicode}
\usepackage[slantfont,boldfont]{xeCJK}

% 设置中文字体
% ==========================================================
\setCJKmainfont[BoldFont=STHeiti,ItalicFont=STKaiti]{STSong}
\setCJKsansfont{STHeiti}
\setCJKmonofont{STFangsong}
 
\setCJKfamilyfont{zhsong}{STSong}
\setCJKfamilyfont{zhhei}{STHeiti}
\setCJKfamilyfont{zhfs}{STFangsong}
\setCJKfamilyfont{zhkai}{STKaiti}
 
\newcommand*{\songti}{\CJKfamily{zhsong}} % 宋体
\newcommand*{\heiti}{\CJKfamily{zhhei}}   % 黑体
\newcommand*{\kaishu}{\CJKfamily{zhkai}}  % 楷书
\newcommand*{\fangsong}{\CJKfamily{zhfs}} % 仿宋
% ==========================================================

%%%%%%%%%%
% the following are user defined commands

\newcommand{\pr}[1]{{\mathbb P}\left(#1\right)}        % probability
\newcommand{\E}[1]{{\mathbb E}\left[#1\right]}        % expectation 
\newcommand{\1}[1]{{\mathbf 1}\left\{#1\right\}}        % indicator
\newcommand{\V}[1]{\text{Var}\left(#1\right)}    % variance

\def\lp{\left(}
\def\rp{\right)}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{exercise}[theorem]{Exercise}
\newtheorem{example}[theorem]{Example}

%%%%%%%%%%%
%%%%%%%%%%%

\title{\scshape STAT 215B Final Project, Spring 2012}
\author{Christine Kuang, Siqi Wu, and Angie Zhu}
\date{\today} % delete this line to display the current date

%%% BEGIN DOCUMENT
\begin{document}
\setlength\footskip{0.5in}


%%% the following is for including source code. Don't worry about it for now. --AZ
\lstset{
% backgroundcolor=\color{Gray} % requires package color
%frame=double,
showspaces=false, 
language=R, 
basicstyle=\ttfamily, 
tabsize=3, 
showstringspaces=false, 
columns=flexible%, 
%numbers=left, 
%numberstyle=\footnotesize, 
%stepnumber=5, 
%numbersep=6pt  % how far the line-numbers are from the code
}

\maketitle

%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%
% To Christine:
% Don't worry about the above part. The report starts from here. 
% Comments are preceded by a percentage symbol.
% LaTeX is a markup language like HTML. All the formatting is specified by particular code. 
% The structure of the report is identified by \section{}, \subsection{}, \subsubsection{}, etc. 

% I made some slides for the Productivity Seminar last Spring: http://www.stat.berkeley.edu/~luis/seminar/IntroToLaTeXSlides_Angie_Zhu.pdf
% It's very short. I have some longer intro material if you are interested.
% It covers some basic rules of LaTeX.

% I will put more reminders here for your reference :D


%% Double quotation marks: require 4 charaters ``'' (a left and a right pair of single quotes, [`] on keyboard is the one right next to [1])



%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%
\section{Introduction}

the largest microblogging website in China, Sina Weibo 新浪微博

A post can be text, an image, video, or other multimedia.

related works here 
%%%%%%%%%%%%%%%%%%
\section{Methods}



%%%%%%%
\subsection{Data Collection}




%%%%%%%
\subsection{Tagging}

%Tagging Limitations

In all, we tagged a total of 4000 Weibo posts.  We made a search for the topic we were looking into and did this over time to get a total of 10,000 Weibo posts, from which we picked out 3000 posts as our training set.  We each tagged 1000 posts.  The final 1000 posts were for our test set and we tagged these together. We had a total of four different categories that we tagged the posts as - neutral, positive, negative, and irrelevant (spam).  

As we each tagged the posts, we encountered and realized a few of the limitations involved. One limitation was in the fact that tagging these posts produced subjective responses.  What one of us read as a negative response to the topic we chose, another may have read as a positive response.  For example, the English phrase 'that wasn't too bad' could be taken as positive or negative.  Positive - the experience was better than expected; negative - the experience wasn't great. Hence, it is difficult sometimes to truly know whether the original author of the post had in mind a negative or positive reaction to the topic he was posting about.  

Another limitation was that some posts we were not sure how to tag. Many posts consisted of just a quote by the author we were looking into.  These could have been seen as positive posts since the writer may have liked the quote and so posted it. But at the same time, the author could have been neutral and was just merely using the quote to apply to a specific circumstance in his life at the time. We were not entirely sure of how to tag each of the posts that fell into this category, and so again, the subjectiveness of tagging the posts by hand comes into play as a limitation in the forming of our model.  Another uncertainty that occurred in trying to manually tag the posts was that some of the posts didn't talk about our chosen topic specifically, but a related topic.  With our chosen topic, people who had negative responses towards 韩寒 (Hanhan), were usually on the side of the opposing author who was discrediting him.  Some of the posts did not directly mention 韩寒, but would instead show support for the opposing side. With these posts, we typically labeled as a negative response.  However, an argument could be made for just throwing out those posts since they do not directly say anything about 韩寒, and they could possibly just be supporting the opposing author in his own literary works and not necessarily in his stance against 韩寒.  Another uncertainty in tagging is what to tag posts that have no subject.  There were a few posts that had nothing to do with the chosen topic at all, but there were also posts that had no subject but contained phrases such as "Keep it up!", "always a supporter!", etc. that could very well be taken as positive posts since our search is for our specific topic. But because there is no subject in the posts, this cannot be taken with 100\% certainty. In these instances, where there is no subject in the posts, we marked them as irrelevant to be on the conservative side in our predictions. 

These limitations in tagging will affect our model's accuracy in predicting whether a post contains a positive or negative response to the topic at hand.  These limitations also indicate to us that in general, models for predicting whether or not a post is negative or positive towards a chosen topic is limited greatly by the subjectiveness of the sentence's meaning/interpretation which affects the training set used to form the model. This limitation could present a potential future question to look into on how we can improve tagging - whether we should just remove posts that have too subjective of an interpretation to tag or if by studying these types of ambiguous posts, we can create certain priors to help in the tagging process. 

%%%%%%%
\subsection{Processing}



UTF-8 encoding, GBK, Unicode, Big5

%%
\subsubsection{Characteristics of Chinese Language}\label{subsec:Chinese}
No explicit delimiter between words in Chinese texts

OOV

ambiguity 


\cite{wong2009introduction}


There has been quite a bit of research done into natural language processing in the English language, but not much on the Chinese language.  This is due to the fact that the Chinese language contains unique characteristics that makes it difficult to do natural language processing well. 
 
First, the Chinese language, unlike the English language, has no explicit delimiter between words. In the English language, spaces separate words and so to segment a sentence into its appropriate components is not too difficult since one could just separate based on spaces.  The Chinese language however has no such delimiter between words.
 
An additional difficulty in segmenting a sentence into its appropriate components and words is that the Chinese languages made of separate characters where each character has a meaning of its own, but if you combine two or more characters together into a phrase, the meaning can be completely different. These types of ambiguities are typically easily resolved by humans reading the sentence and realizing what would make most sense in terms of the context of the
sentence, but it is not so easy for a computer to preform those types of automatic word/phrase segmentations. For example, 他好吃 - this sentence could be separated in two ways. The first character means 'he'. The other two words can be taken as two different phrases: 'tastes delicious' or 'loves to eat'.  It is obvious that the phrase should be taken as "he loves to eat" because "he tastes delicious" does not make sense but it is difficult to have a computer
automatically recognize which sentence makes more sense. Some words also have more than one meaning. For example, 打 can be used in different ways with different meanings. It can be used in the contexts of playing a sport, hitting a person or object, or playing a game. 

Second, the Chinese language has many OOV words (out of vocabulary).  These are new phrases that are not in dictionaries.  These phrases typically come out of cultural references, current hot topics, acronyms, abbreviations, names/nicknames, or just plain slang words.  Specifically to the posts that we looked into, out of vocabulary words typically occurred in the case of cultural references, where there are nicknames created for some hot topic issue/person/event of the week just popular slang or informal words and phrases used to 
convey an emotion.  Especially since social media posts are more popular with the young adult generation, many of the words used in the posts are popular informal phrases that would not normally be found in dictionaries. Knowledge of these types of OOV words comes out of knowing the current trends in Asian countries and being up to date with the typical language used by the younger generation.  

Third, the Chinese language has two forms - traditional and simplified and it is not a 1-1 correspondence between the two languages which makes it difficult to convert between the two languages for translation or natural language processing. 



%%
\subsubsection{Characteristics of Sina Weibo Posts}\label{subsec:Weibo}

Our analysis takes not only the characteristics of Chinese language, but also the characteristics of Sina Weibo posts into consideration.

The writing of Weibo posts are generally informal. The users may not use standard punctuation marks for separation of sentences and parts of sentences. The most important features are described as follows:

\begin{description}
\item[Reposting] A user may repost other post. Reposting does not automatically imply agreement or liking. This type of post usually consists of two parts: the reposting user's comment and the post being reposted. The user's comment may be empty or set as default text ``Repost'' or ``转发微博'' (``Repost Weibo''). The reposted post itself may include multiple reposting. The topic of keeping track of reposting and identifying agreement or disagreement can be a project itself. In our analysis, only the reposting user's comment is kept.  

\item[Spams] There are a fair amount of spams on Weibo. Some spam posts are identical except for the URL. Hence, URLs are removed and then we check for duplication in the pre-tagging processing step.

\item[Mentioning] A user may mention other users whose usernames are preceded by the \MVAt\  symbol. The mentioned usernames may be an integrated part of the post. We define a set of topic-related usernames and substitute the mentioning of these usernames by the corresponding proper nouns. The other mentioned usernames are removed.

\item[Emotion Symbols and Internet Slangs] Sina Weibo provides the users a set of emotion symbols, which are corresponding words surrounded by square brackets in text. The users may use other emotion symbols, such as {\ttfamily :)} for smile and {\ttfamily T\_T} for crying. Internet slangs are a large part of Out-of-Vocabulary (OOV). Some substitute the characters in a word with the characters which have similar pronunciation, such as ``蜀黍'' (Shu3 Shu2) for ``叔叔'' (Shu1 Shu1, means ``uncle''). Some are Internet popular interjections, such as ``喵了个咪'' (喵: ``meow,'' 了: past tense marker, 个: universal measure word, 咪: ``mew'') which means ``dog my cats.'' 

\item[Topic] Topic words are surrounded by the pound signs {\ttfamily \#} since Chinese language has no explicit delimiter between words. The topic word can be an integrated part of the post. 

\end{description}



%%
\subsubsection{Pre-tagging Processing}

The data set {\ttfamily Han.txt} contains 22,398 posts.

In order to obtain labeled messages for training and testing purpose, the authors manually provided sentiment tags to a data set containing 3000 messages. The three types of sentiment tags used here are positive, negative, and noninformative.

The data need to be cleaned before manual labeling.
A typical post looks like:
\begin{verbatim}
1165303315 2012-04-16 09:55:40  《韩寒收到网友死亡威胁》 (来自 @新浪娱乐) http://t.cn/zOprKap
\end{verbatim}

\begin{enumerate}
\item The user identification number and time stamp are removed.
\item Only the reposting user's comment is kept. The reposted part is removed from further analysis. If the resulting string is empty, it will be eliminated as well.
\item URLs are removed.
\item Duplicates are removed.
\end{enumerate}

The output file {\ttfamily hanhanweibo.txt} consists of 13,070 unique posts. 3000 posts are chosen from this data set and will be manually tagged.



%%
\subsubsection{Pre-segmentation Processing}

As discussed in Section~\ref{subsec:Chinese}, sentences in Chinese are normally strings of Chinese characters without spaces between words. Hence, word segmentation is crucial for our word-based analysis. According to the characteristics of Weibo posts described in Setion~\ref{subsec:Weibo}, the following processing is preformed:
\begin{enumerate}
\item A set of topic-related usernames are defined.  Then the mentioning of these usernames are substituted by the corresponding proper nouns. The other mentioned usernames are removed.
\item A set of emotional symbols and Internet slangs are defined.  Then they are substituted by the corresponding word surrounded by square brackets.
\end{enumerate}

	

%%
\subsubsection{Segmentation}

汉语词法分析系统ICTCLAS (Institute of Computing Technology, Chinese Lexical Analysis System) is a well known Chinese word segmentation system developed by Institute of Computing Technology, Chinese Academy of Sciences \cite{ICTCLAS}. It offers the functionality of  Chinese word segmentation, lexical tagging, named entity recognition, unknown words detection, and the user-defined dictionary. 
The current version is ICTCLAS 2011, which supports GB2312, GBK, UTF8 and several encodings and has precision rate of 98.45\%. 

The Java version of ICTCLAS 2011 preforms word segmentation and lexical tagging on a Linux 32-bit machine. A user-defined dictionary is provided. The entries in this dictionary contains proper nouns and some common Internet slangs. For instance, 微博 (Weibo, wei1 bo2) can be written as 围脖 (wei2 bo2, means ``scarf''). Some users refer Han Han as 韩少 (韩: Han Han's surname, 少: abbreviation of 少爷, which means ``young master of the house''). 

Even with the user-defined dictionary, some appearance of Han Han's name 韩寒 can not be segmented and tagged correctly. This is corrected directly using regular expression.

%%	
\subsubsection{Conjunction Rules}

Lee and Renganathan \cite{lee2011chinese} presented that special consideration should be given to the sentences whose parts are linked by contrasting transitional expressions. In particular, if a sentence contains conjunctions such as ``although'' and  ``but,'' only the part being emphasized will be kept and used to infer the sentiment polarity of this sentence. There are three cases:
\begin{enumerate}
\item Although (part A), (part B).
\item (Part A), but (part B).
\item Although (part A), but (part B).
\end{enumerate}
For each case, only part B will be kept. 

The four words for ``although'' are 虽然, 虽说, 虽, and 尽管. The words for ``but'' are 但, 但是, 不过, 可是, 然而, 只是, 可, 只, 然, and 却. 



%%
\subsubsection{Stop Words and Punctuation Elimination}
Moreover, stop words, non-text strings, and punctuation marks are eliminated. The detailed process is as follows: 
\begin{enumerate}
\item  Remove prepositions, punctuation marks, English character strings, interjections, modal particles, onomatopoeia, and auxiliary words.
\item Remove pre-defined stop words and number strings.
\end{enumerate}
Note that the pre-defined stop words do not contain the following six negation words:  不, 不是, 没有, 没, 无, and 别. These negation words will be used in sentiment score assignment.


%%
\subsubsection{Sentiment Score Assignment}
Dictionary-based sentiment score can provide us some intuitive understanding of the sentiment polarity of the posts. 
The dictionaries are obtained from HowNet \cite{HowNet}. HowNet is an online extralinguistic common-sense knowledge system for the computation of meaning in human language technology.


Each post is examined and the numbers of positive and negative words are recorded. Positive word contributes $+1$ to the sentiment score, whereas negative word contributes $-1$. If there is a negation words among the three words before the positive/negative word, their combination will be treated as an entity and their updated contribution is $-1$ times the original contribution. The six negation words used are 不, 不是, 没有, 没, 无, and 别. The sentiment score of the post is the sum of all the contributions. 

Also topic-related positive/negative words are added to the dictionaries. For instance, the users who refer Han Han as 韩少 (韩: Han Han's surname, 少: abbreviation of 少爷, which means ``young master of the house'') clearly have positive feelings about him. 

Another interesting quantities are the numbers of positive and negative words in a neighborhood of a particular person. The neighborhood used in our analysis is three words before and after the person's name.


%%%%%%%
\subsection{Feature analysis}
It is of interest to study the relation between features, i.e. in our case, the relation between words in tweets. We shall base our study on the frequency matrix $X\in R^{n\time p}$, where the entry of the matrix $x_{ij}$ stores the times of occurence of the $j$-th word in the $i$-th post. Analyzing the feature matrix $X$ can help us understand the word usage better and identify possible cluster in feature space. 

Our observation matrix is the frequency matrix of dimension 756x3000. The columns represent each post and the rows represent 756 phrases.  These 756 phrases have total occurance of at least 10 times over all 3000 posts and do not include stop words and negation words. Each element of the observation matrix represents the total number of times that particular word (indicated by row) occurrs in that post (indicated by column). Using 10 fold cross validation, we searched for the optimal regularization parameter. 

plot the frequency matrix here.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Exploratory data analysis}
converting frequency matrix to cooccurence matrix. 

include plots of the co matrix, both matrixplot and the network plot 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Sparse principal component analysis (SPCA)}
Following Zou et al (2006):
\begin{align}
(A,B) & = \arg \min_{A,B} \sum_{i=1}^n||x_i-AB^Tx_i||_2^2 + \lambda \sum_{j=1}^k||\beta_j||^2 + \sum_{j=1}^k\lambda_{1,j}||\beta_j||_1 \\ 
\text{subject to} & A^TA = I_{k}. \nonumber
\end{align}

table here. Some explanations here as well.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Sparse gaussian graphical model}
Some review on a Gaussian graphical model. maximum likelihood estimate of the inverse covariance matrix. 
Suppose the random vector $x\in R^p$ has a multivariate normal distribution with mean $\mu$ and covariance matrix $\Sigma$. The density function of $x$ is
\begin{align}
\label{eq: normalPDF}
f(x|\mu,\Sigma) = \frac{1}{\sqrt{2\pi \det \Sigma}}\exp{-\frac{(x-\mu)^T\Sigma^{-1}(x-\mu)}{2}}.
\end{align}
Suppose we have IID $N(\nu,\Sigma)$ data $x_1,x_2,...x_n$ and we want to estimate the inverse covariance matrix $\Sigma^{-1}$. The joint likelihood of the data is
\begin{align}
\label{eq: like}
f(x|\mu,\Sigma) = \frac{1}{(2\pi \det \Sigma)^{n/2}}\exp{-\frac{\sum_{i=1}^n(x_i-\mu)^T\Sigma^{-1}(x_i-\mu)}{2}}.
\end{align}
Taking logarithm to get the log-likelihood (and ignore constant terms):
\begin{align}
\label{eq: loglike}
l(\mu,\Sigma^{-1}) = \frac{1}{2\log \det \Sigma}  -\frac{\sum_{i=1}^n(x_i-\mu)^T\Sigma^{-1}(x_i-\mu)}{2}.
\end{align}
Then we can do a maximum likelihood estimation (optimize over $\mu$ and $S = \Sigma^{-1}$; easy to see that the MLE for $\mu$ is $\bar{X}$ exponential family, MOM = MLE, so just plug in $\bar{X}$ for $\mu$):

\begin{align}
\label{eq: loglike}
\max_S l(S) = \frac{1}{2}\log \det S  -\frac{\sum_{i=1}^n(x_i-\mu)^T S (x_i-\mu)}{2}.
\end{align}

Here comes the trace trick $\sum_{i=1}^n(x_i-\mu)^T S (x_i-\mu) = \textbf{Tr} (\sum_{i=1}^n (x_i-\mu)(x_i-\mu)^TS) = \textbf{Tr}(\hat{\Sigma}S)$. We end up with the following optimization problem

\begin{align}
\label{eq:mle}
\max_S \log \det S - \textbf{Tr}\hat{\Sigma}S 
\end{align}

if the number of features $p$ is large, want to do model selection... So Banerjee et al. (2007) propose the following optimization problem to recover the sparse structure in a gaussian graphical model
\begin{align}
\label{eq:gLasso}
\max_S \log \det S - \textbf{Tr}\hat{\Sigma}S - \lambda ||S||_1
\end{align}
where $\Sigma$ is the covariance matrix of the data/design matrix $X$ and $||S||_1 = \sum_{i=1}\sum_{j=1} |s_{ij}|$. In that paper they study the senate voting data to recover the party membership. l-1 norm promotes sparsity. 

Some computation aspects of estimating a sparse graphical model: Banerjee et al (2007) propose a block coordinate ascent method (COVSEL) (updating one row and one column of $S$ at one time). Their approach is exact but takes forever to run. Meinshausen and Buhlmann (2006) uses an approximation approach that is substantially faster. In our study, we adopt the fast and accurate graphical LASSO procedure (R package glasso) due to Friedman et al (2007).  

p
%%%%%
\subsection{Classification}
We are also interested in classifying the posts into different categories. Let $x_i\in R^{p}$ be the $i$-th row of the frequency matrix $X$ and $y_i$ the corresponding category. For simplicity, let us assume that $y_i\in\{-1,+1\}$ is binary, where the ``+1'' can be used to label the following four categories: 1) positive opinion towards Han Han; 2) negative opinion towards Han Han; 3) netural or unidentifiable opinion; 4) spam and ``-1'' labels the complement of the inividual category (e.g. if ``+1'' means positive, ``-1'' would mean anything but positive. Note that the complement of positive is not negative, but rather the union of negative, neutral and spam). For each of the above four cases, we apply LASSO and $l_1$-norm support vector machine to classify the data points into ``-1'' and ``+1''.  

For each of the four models, we changed all responses that were not the category we were modeling to -1 and responses that fell into the category we were looking into as 1.  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Sparse regression with the LASSO}
The LASSO (Tibshirani, 1996) is a popular sparse regression method which adds a $l_1$-norm penalty to the linear least squares problem to promote sparsity in the regression coefficients:
\begin{align}
\label{eq:Lasso}
\hat{\beta}(\lambda) = \arg \min_\beta \frac{1}{2}||y-(\beta_0+X\beta)||_2^2 + \lambda ||\beta||_1
\end{align}
In our study, we regress the class label vector $y$ onto the word frequency matrix $X$, yielding the intercept $\hat{\beta_0}$ and the sparse regression vector $\hat{\beta}(\lambda)$. We can then define the classifier to be $f(x) = \textbf{sign}(\hat{\beta_0}+X\hat{\beta}(\lambda))\in\{-1,+1\}$, where the resulting coefficient regression coefficient $\hat{\beta}$ has the following explanation: for each feature/word $j$, given all other feature/word variable fixed, the increase of the $j$-th word frequency by one lead to increase in regression function $\beta_0+X\beta$ by an amount of $\beta_i$ (if $\beta_i$ turns out to be positive, this means the chance of classifying the data point into the +1 category is increased).

To look at which words were most relevant to each category we modeled (positive, negative, neutral and spam), we looked at three sets of 20 words based on, respectively, the highest absolute beta values, most positive coefficient values and most negative coefficient values. The top 30 words based on the top 50 highest absolute beta values tells us in general which words were most relevant to predicting that particular category.  The top 50 words based on the top 50 highest positive beta values tells us which words typically were common in posts that fell into the category observed while the top 50 words based on the top 50 highest negative beta values tells us which words typically were not in posts that fell into the category observed but were rather more common in the categories outside of the one we were modeling.

Since the estimated $\beta$ depend on the regularization parameter, we are left with the issue of choose the ``best'' $\lambda$. A commonly used approach is to do a grid search for $\lambda$: for each value of $\lambda$, do a 10-fold cross validation; then choose the $\lambda$ that yields the smallest cross validation testint sample error. For this purpose, we used the approach least angle regression by Efron and Hastie (2007) to do the model selection. Their R package lars efficiently fits an entire lasso sequence with the least squares loss function. 

include the one of the CV plot here; and include the others in the appendix.

For the positive responses, the top 20 most relevant words (based on taking the absolute values of the betas) returned by our model are shown in the table. As can be seen, the top 20 relevant words have an assortment of fairly neutral or positive words. The top 20 most relevant words for just the positive betas resulted in words that were very positive in nature. For example, words such as 'mature', 'support', and 'keep going' are very positive in nature and would certainly indicate a positive reaction to 韩寒. The top 20 most relevant words for just the negative betas resulted in words that showed a great dislike for 韩寒.  Words such as 'liar' indicate a negative response to the author. 

For the negative responses, the top 20 most relevant words (based on taking the absolute values of the betas) returned by our model are shown in the table. As can be seen, the words are typically negative or neutral.  The top 20 most relevant words for just the positive betas are very telling in the posts sentiment towards 韩寒. Words such as 'disgusting, 'hate', 'liar' and 'annoying' demonstrates easily that the post has a negative sentiment towards the author. The top 20 most relevant words for just the negative betas are hence typically more positive towards the author. With words such as 'support', 'good', and 'like', the posts would not have a negative sentiment. What is interesting to note with these betas is the fact that these betas are all close to zero except the highest phrase. 

For the neutral responses, the top 20 most relevant words (based on taking the absolute values of the betas) as well as the top 20 most relevant words based on the positive betas are all neutral words. The top 20 most relevant words based on the negative betas consist mainly of phrases that have some clear emotion attached to them, such as "support", "hate", "agree", and "ghostwriter". For the spam posts, the top 20 most relevant words (based on taking the absolute values of the betas) as well as the top most relevant words based on the positive betas are words/phrases that have no relation whatsoever with the author we picked to look into.  As expected, the top 20 most relevant words based on the negative betas are words/phrases that do have to do with the topic, such as the author's name. 

for the spam responses...
change 20 to 5, and say the complete lists of words can be found in the appendix
\begin{table}
\caption{Positive category}
\begin{center}
\begin{tabular}{|c|c||c|c||c|c|}
\hline
Word & Absolute Coef. & Word & Positive Coef. & Word & Negative Coef.\\ \hline \hline
加油 & 0.820 & 加油 & 0.820 & 样子 & -0.396\\
(keep going) & & (keep going) & & (manner) & \\\hline
韩少 & 0.644 & 韩少 & 0.644 & 恋 & -0.344\\
(Master Han) & & (Master Han) & & (love) & \\\hline
成熟 & 0.546 & 成熟 & 0.546 & 发表 & -0.336\\
(mature) & & (mature) & & (announce) & \\\hline
顶 & 0.533 & 顶 & 0.533 & 道理 & -0.336\\
(support) & & (support) & & (rational) & \\\hline
宽容 & 0.518 & 宽容 & 0.518 & 利益 & -0.335\\
(tolerant) & & (tolerant) & & (benefit) & \\\hline
\end{tabular}
\end{center}
\end{table}



\begin{table}
\caption{Negative category}
\begin{center}
\begin{tabular}{|c|c||c|c||c|c|}
\hline
Word & Absolute Coef. & Word & Positive Coef. & Word & Negative Coef.\\ \hline \hline
讨厌 & 0.481 & 讨厌 & 0.481 & 支持 & -0.008\\
(hate) & & (hate) & & (support) & \\\hline
无耻 & 0.412 & 无耻 & 0.412 & 不 & 0.000\\
(shameless) & & (shameless) & & (no) & \\\hline
恶心 & 0.395 & 恶心 & 0.395 & 人 & 0.000\\
(disgusting) & & (disgusting) & & (people/person) & \\\hline
骗子 & 0.380 & 骗子 & 0.380 & 说 & 0.000\\
(liar) & & (liar) & & (say) & \\\hline
扁 & 0.353 & 扁 & 0.353 & 方舟子 & 0.000\\
(beat up) & & (beat up) & & (FangZhouZi) & \\\hline
\begin{table}

\caption{Neutral category}
\begin{center}
\begin{tabular}{|c|c||c|c||c|c|}
\hline
Word & Absolute Coef. & Word & Positive Coef. & Word & Negative Coef.\\ \hline
上调 & 0.586 & 上调 & 0.586 & 加油 & -0.491\\
(increase) & & (increase) & & (keep going) & \\\hline
道理 & 0.566 & 道理 & 0.566 & 韩少 & -0.358\\
(rational) & & (rational) & & (Master Han) & \\\hline
账号 & 0.534 & 账号 & 0.534 & 苦肉计 & -0.327\\
(account) & & (account) & & (the ruse of  & \\
& &  & &  self-injury to win & \\
& &  & &  somebody's & \\
& &  & &   confidence) & \\\hline
加油 & 0.491 & 铁证 & 0.459 & 支持 & -0.290\\
(keep going) & & (clear evidence) & & (support) & \\\hline
铁证 & 0.459 & 称 & 0.453 & 善良 & -0.268\\
(clear evidence) & & (refer) & & (kind) & \\\hline
\end{tabular}
\end{center}
\end{table}


\begin{table}
\caption{Spam category}
\begin{center}
\begin{tabular}{|c|c||c|c||c|c|}
\hline
Word & Absolute Coef. & Word & Positive Coef. & Word & Negative Coef.\\ \hline
查看 & 3.777 & 查看 & 3.777 & 韩少 & -1.033\\
(examine) & & (examine) & & (Master Hanhan) & \\\hline
抽 & 1.998 & 抽 & 1.998 & 韩寒 & -0.716\\
(win) & & (win) & & (Master Hanhan) & \\\hline
每天 & 1.251 & 每天 & 1.251 & 别 & -0.232\\
(everyday) & & (everyday) & & (don't) & \\\hline
往往 & 1.208 & 往往 & 1.208 & 支持 & -0.217\\
(often) & & (often) & & (support) & \\\hline
外 & 1.043 & 外 & 1.043 & 这种 & -0.202\\
(outside) & & (outside) & & (this kind) & \\\hline
\end{tabular}
\end{center}
\end{table}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{$l_1$-norm support vector machine}
Support vector machine (Vapnik 1996) is another commonly-used machine learning method to classify data points into two categories. Consider again the linear decision function $f(x) = \beta_0 + \beta x$ and the classifier $Class(x) = \textbf{sign} (f(x))$. To ``learn'' the parameters, we want the training misclassification rate to be small and the margin of the decision boundary (which can be shown to be $1/||\beta||_2$) to be wide. Hence we consider the following optimization problem (give a 2d illustration here...):   
\begin{align}
\label{eq:l2svm}
\min_{\beta_0,\beta} \sum_{i=1}^n(1-y_i(\beta_0+\beta^Tx_i))_+ + \frac{\lambda}{2} ||\beta||_2,
\end{align}
where $z_+ = \max(0,z)$ (the function $h(z) = (1-z)_+$ is also known as the hinge loss function). Similarly, the sparse version of SVM simply replaces the $l_2$-norm by $l_1$-norm (which is just another measure of the wideness of the margin):
\begin{align}
\label{eq:l1svm}
\min_{\beta_0,\beta} \sum_{i=1}^n(1-y_i(\beta_0+\beta^Tx_i))_+ + \lambda ||\beta||_1.
\end{align}
We repeat the same data analysis as we did for the LASSO method. The results are summaried in table XX. For efficiently fitting the sparse svm, we use the matlab package by Fung and Mangasarian (2004). Again, 10-fold cross validations are performed in order to select the ``best'' $\lambda$.

include the four tables here(each of top five words)... and rest in the appendix

%%%%%%%%%%%%%%%%%%
\section{Appendix}

%%%%%

\begin{table}
\caption{Positive category}
\begin{center}
\begin{tabular}{|c|c||c|c||c|c|}
\hline
Word & Absolute Coef. & Word & Positive Coef. & Word & Negative Coef.\\ \hline \hline
加油 & 0.820 & 加油 & 0.820 & 样子 & -0.396\\
(keep going) & & (keep going) & & (manner) & \\\hline
韩少 & 0.644 & 韩少 & 0.644 & 恋 & -0.344\\
(Master Han) & & (Master Han) & & (love) & \\\hline
成熟 & 0.546 & 成熟 & 0.546 & 发表 & -0.336\\
(mature) & & (mature) & & (announce) & \\\hline
顶 & 0.533 & 顶 & 0.533 & 道理 & -0.336\\
(support) & & (support) & & (rational) & \\\hline
宽容 & 0.518 & 宽容 & 0.518 & 利益 & -0.335\\
(tolerant) & & (tolerant) & & (benefit) & \\\hline
支持 & 0.477 & 支持 & 0.477 & 称 & -0.323\\ \hline
家人 & 0.467 & 家人 & 0.467 & 遭受 & -0.323\\ \hline
样子 & 0.396 & 尤其 & 0.395 & 媒体 & -0.319\\ \hline
尤其 & 0.395 & 欣赏 & 0.383 & 翻 & -0.314\\ \hline
欣赏 & 0.383 & 感动 & 0.381 & 铁证 & -0.289\\ \hline
感动 & 0.381 & 影响力 & 0.370 & 骗子 & -0.248\\ \hline
影响力 & 0.370 & 新书 & 0.327 & 上调 & -0.248\\ \hline
恋 & 0.344 & 铁 & 0.316 & 投票 & -0.234\\ \hline
发表 & 0.336 & 不错 & 0.309 & 女 & -0.230\\ \hline
道理 & 0.336 & 终于 & 0.274 & 四娘 & -0.226\\ \hline
利益 & 0.335 & 每个 & 0.274 & 关系 & -0.215\\ \hline
新书 & 0.327 & 咬 & 0.261 & 广告 & -0.210\\ \hline
称 & 0.323 & 文字 & 0.260 & 接受 & -0.208\\ \hline
遭受 & 0.323 & 蛋 & 0.244 & 网 & -0.204\\ \hline
媒体 & 0.319 & 纠缠 & 0.244 & 底 & -0.196\\ \hline
\end{tabular}
\end{center}
\end{table}



\begin{table}
\caption{Negative category}
\begin{center}
\begin{tabular}{|c|c||c|c||c|c|}
\hline
Word & Absolute Coef. & Word & Positive Coef. & Word & Negative Coef.\\ \hline \hline
讨厌 & 0.481 & 讨厌 & 0.481 & 支持 & -0.008\\
(hate) & & (hate) & & (support) & \\\hline
无耻 & 0.412 & 无耻 & 0.412 & 不 & 0.000\\
(shameless) & & (shameless) & & (no) & \\\hline
恶心 & 0.395 & 恶心 & 0.395 & 人 & 0.000\\
(disgusting) & & (disgusting) & & (people/person) & \\\hline
骗子 & 0.380 & 骗子 & 0.380 & 说 & 0.000\\
(liar) & & (liar) & & (say) & \\\hline
扁 & 0.353 & 扁 & 0.353 & 方舟子 & 0.000\\
(beat up) & & (beat up) & & (FangZhouZi) & \\\hline
装 & 0.321 & 装 & 0.321 & 韩少 & 0.000\\ \hline
选项 & 0.292 & 选项 & 0.292 & 真 & 0.000\\ \hline
苦肉计 & 0.290 & 苦肉计 & 0.290 & 好 & 0.000\\ \hline
利益 & 0.283 & 利益 & 0.283 & 没 & 0.000\\ \hline
全 & 0.261 & 全 & 0.261 & 一个 & 0.000\\ \hline
国家 & 0.247 & 国家 & 0.247 & 微博 & 0.000\\ \hline
智商 & 0.216 & 智商 & 0.216 & 写 & 0.000\\ \hline
告 & 0.198 & 告 & 0.198 & 喜欢 & 0.000\\ \hline
虚伪 & 0.192 & 虚伪 & 0.192 & 想 & 0.000\\ \hline
演 & 0.191 & 演 & 0.191 & 威胁 & 0.000\\ \hline
语 & 0.186 & 语 & 0.186 & 只 & 0.000\\ \hline
烦 & 0.178 & 烦 & 0.178 & 太 & 0.000\\ \hline
掉 & 0.142 & 掉 & 0.142 & 事 & 0.000\\ \hline
下去 & 0.141 & 下去 & 0.141 & 没有 & 0.000\\ \hline
公开 & 0.141 & 公开 & 0.141 & 看到 & 0.000\\ \hline
\end{tabular}
\end{center}
\end{table}


\begin{table}
\caption{Neutral category}
\begin{center}
\begin{tabular}{|c|c||c|c||c|c|}
\hline
Word & Absolute Coef. & Word & Positive Coef. & Word & Negative Coef.\\ \hline
上调 & 0.586 & 上调 & 0.586 & 加油 & -0.491\\
(increase) & & (increase) & & (keep going) & \\\hline
道理 & 0.566 & 道理 & 0.566 & 韩少 & -0.358\\
(rational) & & (rational) & & (Master Han) & \\\hline
账号 & 0.534 & 账号 & 0.534 & 苦肉计 & -0.327\\
(account) & & (account) & & (the ruse of  & \\
& &  & &  self-injury to win & \\
& &  & &  somebody's & \\
& &  & &   confidence) & \\\hline
加油 & 0.491 & 铁证 & 0.459 & 支持 & -0.290\\
(keep going) & & (clear evidence) & & (support) & \\\hline
铁证 & 0.459 & 称 & 0.453 & 善良 & -0.268\\
(clear evidence) & & (refer) & & (kind) & \\\hline
称 & 0.453 & 想起 & 0.353 & 成熟 & -0.263\\ \hline
韩少 & 0.358 & 杀 & 0.331 & 终于 & -0.239\\ \hline
想起 & 0.353 & 最终 & 0.329 & 家人 & -0.233\\ \hline
杀 & 0.331 & 意思 & 0.323 & 同意 & -0.228\\ \hline
最终 & 0.329 & 遭遇 & 0.319 & 越来越 & -0.220\\ \hline
苦肉计 & 0.327 & 金 & 0.308 & 欢乐 & -0.217\\ \hline
意思 & 0.323 & 片 & 0.287 & 崇拜 & -0.213\\ \hline
遭遇 & 0.319 & 应 & 0.278 & 讨厌 & -0.202\\ \hline
金 & 0.308 & 变成 & 0.265 & 顶 & -0.197\\ \hline
支持 & 0.290 & 有点 & 0.262 & 代笔 & -0.194\\ \hline
片 & 0.287 & 之间 & 0.254 & 跳 & -0.191\\ \hline
应 & 0.278 & 右边 & 0.250 & 真善美 & -0.186\\ \hline
善良 & 0.268 & 民主 & 0.238 & 真正 & -0.185\\ \hline
变成 & 0.265 & 郭敬明 & 0.237 & 欣赏 & -0.181\\ \hline
成熟 & 0.263 & 久 & 0.226 & 无耻 & -0.180\\ \hline
\end{tabular}
\end{center}
\end{table}


\begin{table}
\caption{Spam category}
\begin{center}
\begin{tabular}{|c|c||c|c||c|c|}
\hline
Word & Absolute Coef. & Word & Positive Coef. & Word & Negative Coef.\\ \hline
查看 & 3.777 & 查看 & 3.777 & 韩少 & -1.033\\
(examine) & & (examine) & & (Master Hanhan) & \\\hline
抽 & 1.998 & 抽 & 1.998 & 韩寒 & -0.716\\
(win) & & (win) & & (Master Hanhan) & \\\hline
每天 & 1.251 & 每天 & 1.251 & 别 & -0.232\\
(everyday) & & (everyday) & & (don't) & \\\hline
往往 & 1.208 & 往往 & 1.208 & 支持 & -0.217\\
(often) & & (often) & & (support) & \\\hline
外 & 1.043 & 外 & 1.043 & 这种 & -0.202\\
(outside) & & (outside) & & (this kind) & \\\hline
韩少 & 1.033 & 征集 & 0.948 & 感 & -0.196\\ \hline
征集 & 0.948 & 容 & 0.849 & 韓 & -0.191\\ \hline
容 & 0.849 & 风 & 0.649 & 没有 & -0.179\\ \hline
韩寒 & 0.716 & 票子 & 0.570 & 上调 & -0.174\\ \hline
风 & 0.649 & 考 & 0.540 & 方舟子 & -0.160\\ \hline
票子 & 0.570 & 主 & 0.438 & 光明 & -0.141\\ \hline
考 & 0.540 & 性 & 0.430 & 一定 & -0.137\\ \hline
主 & 0.438 & 总是 & 0.416 & 照妖镜 & -0.132\\ \hline
性 & 0.430 & 儿 & 0.416 & 写 & -0.132\\ \hline
总是 & 0.416 & 结论 & 0.405 & 觉得 & -0.119\\ \hline
儿 & 0.416 & 后面 & 0.397 & 甚 & -0.110\\ \hline
结论 & 0.405 & 法律 & 0.388 & 韩 & -0.092\\ \hline
后面 & 0.397 & 机会 & 0.376 & 真相 & -0.084\\ \hline
法律 & 0.388 & 公知 & 0.357 & 挺 & -0.072\\ \hline
机会 & 0.376 & 中 & 0.357 & 不 & -0.068\\ \hline
\end{tabular}
\end{center}
\end{table}

%%%%%%%%%%%%%%%%%%
\section{Discussion}

ROC curve
precision and recall curve

%%
\subsection{Limitations}

sampling \cite{boyd2004fastest} \cite{leskovec2006sampling}  \cite{wang2011understanding}

reposting

other language, such as English

simplified Chinese and traditional Chinese: no simple one-to-one correspondence; word segmentation and then substitute words



%%%%%%%%%%%%%%%%%%
\section{Conclusion}



%
%\begin{center}
%\begin{figure}[tb]
%   \centering
%   \includegraphics[width=\textwidth]{.png} 
%      \caption{}
%   \label{fig:}
%\end{figure}
%\end{center}


\newpage
%%%%%%%%%%%%%
% bibliography
\bibliographystyle{acm}
\bibliography{215B_FinalProjRef}




%%%%%
%\appendix
%The complete source code is as follows:
%\lstinputlisting{}


\end{document}
